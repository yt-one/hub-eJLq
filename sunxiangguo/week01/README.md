📋 四种文本分类模型对比 + 通用流程说明
## 四种模型对比（KNN / 朴素贝叶斯 / 逻辑回归 / 线性 SVM）
| 模型 | 优点 | 缺点 | 适用场景 |
|------|------|------|----------|
| KNN（K-近邻） | 原理简单，易理解；无需显式训练，只是记住训练数据；能处理多分类 | 预测时需和所有样本算距离，高维稀疏文本特征下速度慢、内存占用大；对特征缩放和噪声敏感；不适合大规模数据 | 数据量小、想要快速上手做实验；对实时性要求不高的离线小任务 |
| MultinomialNB（多项式朴素贝叶斯） | 训练和预测都很快；对高维稀疏文本特征表现稳健；在小数据集上往往效果不错；参数少，不易过拟合 | 假设“特征条件独立”，在语义依赖强的场景上偏简单；对特征工程较敏感；概率估计有时偏差较大 | 短文本分类、垃圾邮件检测、情感分析等常见文本任务；作为强基线模型 |
| LogisticRegression（逻辑回归） | 线性模型，在高维稀疏文本上效果稳定；可输出概率（`predict_proba`）；参数不多，易于调参和正则化 | 本质是线性分类器，对复杂非线性关系建模能力有限；在大数据上训练时间比 NB 长一些；需要足够迭代次数收敛 | 想要概率输出、结果可解释性强、需要稳定性能的文本分类任务 |
| LinearSVC（线性支持向量机） | 在很多文本分类任务上表现非常强，有时略优于逻辑回归；对高维稀疏数据友好；支持 `class_weight` 处理类别不平衡 | 不直接输出概率（需要额外校准）；对参数（尤其是 `C`）比较敏感；大数据上训练时间可能较长 | 对准确率要求高、特征维度高（如 TF-IDF / 词袋）、且希望使用线性模型的场景 |
简单总结：
- 想最简单、不关心性能：KNN（只玩玩）
- 想快、稳、强基线：MultinomialNB
- 想概率 + 可解释：LogisticRegression
- 想尽可能压榨线性模型性能：LinearSVC
---
